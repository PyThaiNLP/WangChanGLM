{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2c077d-9bf4-414b-a181-d9540f110126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b28e32-0148-4136-968f-eadef4f40a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained = \"pythainlp/wangchanglm-7.5B-sft-adapter-merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82b6c91-5447-4641-ad58-6ffdd7775f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2d2c3-ace6-46e5-942c-bb400fed806e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d13cf5b-e316-4623-a8fa-9af3714f876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd3c0c1-02d4-4551-869e-91b7cc4a1b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d58bc7-5a53-4d5c-a5db-116ce39dee95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# generate_text = pipeline(model=pretrained, torch_dtype=torch.float16,\n",
    "#                          trust_remote_code=True, device_map=\"auto\", return_full_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6220791-9645-411e-a437-66cbc5b3d051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef9432feee14f01872527b6bef093df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "hf_pipeline = HuggingFacePipeline.from_model_id(model_id=pretrained, task=\"text-generation\", model_kwargs={\"temperature\":0, \"max_length\":128})\n",
    "#HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d71d62-d994-49c7-b914-4a994c6c1103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a5d6f2-0a87-491c-8e39-e54ae3c4a8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (128) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ดอก โดยใช้หลักการของแสงและคลอโรฟิลล์ อธิบายกระบวนการสังเคราะห์แสงของพืชดอก โดยใช้หลักการของแสงและคลอโรฟิลล์ อธิบายกระบวนการสังเคราะห์แสงของพืชดอก โดยใช้หลักการของแสงและคลอโรฟิลล์ อธิบายกระบวนการสังเคราะห์แสงของพืชดอก โดยใช้หลักการของแสงและคลอโรฟิลล์ อธิบายกระบวนการสังเคราะห์แสงของพืชดอก โดยใช้หลักการของแสงและคลอโรฟิลล์ อธิบายกระบวนการสังเคราะห์แสงของพืชดอก โดยใช้หลักการของแสงและคลอโรฟิลล์ อธิบายกระบวนการสังเคราะห์แสงของ\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.predict(instruction=\"จงอธิบายกระบวนการสังเคราะห์แสงของพืช\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390ac2f3-dc29-4468-89ed-5b5f70c9513b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bot>: Fusion is a process in which two atoms combine to form a third atom. This is a very energetic process, and it is very difficult to do. Fusion is the process of creating a third atom from two atoms. For example, if you have two hydrogen atoms, you can combine them to form a helium atom. This is called fusion.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabbe7aa-eb7d-4c40-aba9-590378dcb719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was the first president to be born in the new nation. He was also the first president to be born in the new nation. He was also the first president to be born in the new nation. He was also the first president to be born in the new nation. He was also the first president to be born in the new nation. He was also the first\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"George Washington (February 22, 1732[b] – December 14, 1799) was an American military officer, statesman,\n",
    "and Founding Father who served as the first president of the United States from 1789 to 1797.\"\"\"\n",
    "\n",
    "print(llm_context_chain.predict(instruction=\"When was George Washington president?\", context=context).lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab028e-11fc-4ea9-b2b4-5ef760ed5683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
