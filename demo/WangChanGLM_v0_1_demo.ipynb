{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Sritb3XEwMpb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73cbe33ab9d64a9388056851ce90197e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdd451d0677942e5b537c22fe3d63493",
              "IPY_MODEL_81369c877e4a45f3914aa9ec3fd792f5",
              "IPY_MODEL_53790cae4cbe4f8fadd1f3707c5a5491"
            ],
            "layout": "IPY_MODEL_af76d00bf2a44b3399ef101461c9b095"
          }
        },
        "cdd451d0677942e5b537c22fe3d63493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bd6171fa434b999868464c60bfdc15",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa721198ba94a9492b273c30615e646",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "81369c877e4a45f3914aa9ec3fd792f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_927874c8375e4af3ba07855530699a38",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39dfab5b0c3d4da385d8981626b49b8d",
            "value": 98
          }
        },
        "53790cae4cbe4f8fadd1f3707c5a5491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f0a8afb3984626965b0c28382f932d",
            "placeholder": "​",
            "style": "IPY_MODEL_13f8f489f7b1440b8c792933f9100815",
            "value": " 98/98 [03:34&lt;00:00,  7.86s/it]"
          }
        },
        "af76d00bf2a44b3399ef101461c9b095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2bd6171fa434b999868464c60bfdc15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa721198ba94a9492b273c30615e646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "927874c8375e4af3ba07855530699a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39dfab5b0c3d4da385d8981626b49b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37f0a8afb3984626965b0c28382f932d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f8f489f7b1440b8c792933f9100815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🐘 WangChanGLM v0.1 demo\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pythainlp/WangChanGLM/blob/main/demo/WangChanGLM_v0_1_demo.ipynb)\n",
        "\n",
        "- Notebook นี้เขียนตัวอย่างการใช้งานโมเดล WangchanGPT ที่เป็นโมเดลภาษาขนาดใหญ่ (large language model) ที่ถูก fine-tune เพื่อให้ตอบคำถามผ่านการใส่ text prompt ได้\n",
        "- สำหรับรายละเอียดการทำงานและการเทรนโมเดลสามารอ่านเพิ่มเติมได้จาก [Medium blog](https://medium.com/@iwishcognitivedissonance/7aa9a0f51f5f)\n",
        "- Notebook พัฒนาภายใต้ license [Apache Software License 2.0](https://github.com/PyThaiNLP/pythainlp/blob/dev/LICENSE).\n",
        "- Repository: [pythainlp/wangchanglm](https://github.com/PyThaiNLP/WangChanGLM)\n",
        "- วิธีกาารรัน ให้ login บัญชี Google แล้ว\n",
        "  - กดที่แถบ Runtime จากนั้นคลิก Run all\n",
        "  - สำหรับภาษาไทยกดที่แถบ รันไทม์ จากนั้นคลิ้ก เรียกใช้ทุกเซลล์\n",
        "  - คุณสามารถใช้ colab gpu ธรรมดาในการรัน notebook นี้ได้ ไม่จำเป็นต้องสมัคร colab pro\n",
        "\n",
        "ผู้พัฒนา: [PyThaiNLP](https://github.com/PyThaiNLP) และ [VISTEC-depa AI Research Thailand](https://airesearch.in.th/)\n",
        "\n",
        "![](https://avatars.githubusercontent.com/u/32934255?s=45&v=4)\n",
        "![](https://airesearch.in.th/assets/img/logo/airesearch-logo.svg)\n",
        "\n",
        "**ไม่รับประกันคำตอบที่ตอบกลับ**"
      ],
      "metadata": {
        "id": "Ixo7mcoAHiSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install\n",
        "!pip install --q torch sentencepiece accelerate transformers pandas requests gradio bitsandbytes langchain faiss-gpu sentence-transformers gradio"
      ],
      "metadata": {
        "id": "RI6wPaY5a1Rl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf6ee24-d7b3-4e7a-d2c8-dbdc77f3eef7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m666.9/666.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/286.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "#@markdown WangChanGLM v0.1 มี 2 โมเดล คือ\n",
        "#@markdown \n",
        "#@markdown 1. pythainlp/wangchanglm-7.5B-sft-en-sharded -> เป็นโมเดลที่เทรนด้วยภาษาอังกฤษอย่างเดียว (ดีที่สุด)\n",
        "#@markdown 2. pythainlp/wangchanglm-7.5B-sft-enth-sharded -> โมเดลที่เทรนกับชุดข้อมูลที่เป็นภาษาไทย อาจจะดีกว่าโมเดลข้างบนในบางกรณี\n",
        "#@markdown\n",
        "#@markdown **หากคุณเปลี่ยนแปลงโมเดลที่รัน** หลังจากรันข้างล่างหมดแล้ว ให้คุณเลืแกโมเดลที่ต้องการและรันใหม่ตั้งแต่ต้นอีกครั้ง โดยให้กด Runtime -> Disconnect Colab Runtime แล้ว Run all หรือ รันไทม์ -> ยกเลิกการเชื่อมต่อและลบรันไทม์ แล้วคลิก เรียกใช้ทุกเซลล์\n",
        "name_model = \"pythainlp/wangchanglm-7.5B-sft-en-sharded\" #@param [\"pythainlp/wangchanglm-7.5B-sft-en-sharded\",\"pythainlp/wangchanglm-7.5B-sft-enth-sharded\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jpPRZKjxuEgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrKHUyL0WNjX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "73cbe33ab9d64a9388056851ce90197e",
            "cdd451d0677942e5b537c22fe3d63493",
            "81369c877e4a45f3914aa9ec3fd792f5",
            "53790cae4cbe4f8fadd1f3707c5a5491",
            "af76d00bf2a44b3399ef101461c9b095",
            "f2bd6171fa434b999868464c60bfdc15",
            "8fa721198ba94a9492b273c30615e646",
            "927874c8375e4af3ba07855530699a38",
            "39dfab5b0c3d4da385d8981626b49b8d",
            "37f0a8afb3984626965b0c28382f932d",
            "13f8f489f7b1440b8c792933f9100815"
          ]
        },
        "outputId": "e343ee91-77f7-491b-f393-7459aa4da1a6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/98 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73cbe33ab9d64a9388056851ce90197e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Download Model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import Optional, Union, List, Dict, Any\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# name_model = \"pythainlp/wangchanglm-7.5B-sft-adapter-merged-sharded\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    name_model, \n",
        "    return_dict=True, \n",
        "    load_in_8bit=True , # try use load_in_8bit=False for use fp16 :D\n",
        "    device_map=\"auto\", \n",
        "    torch_dtype=torch.float16, \n",
        "    offload_folder=\"./\", \n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-7.5B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## คู่มือการรันเบื้องต้น"
      ],
      "metadata": {
        "id": "Sritb3XEwMpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### กำหนดภาษา\n",
        "\n",
        "เนื่องจาก WangchanGPT เป็นโมเดล Multilingual Instruction-Following Model รองรับหลายภาษา ซึ่งอาจทำให้บางครั้งคำถามภาษาไทยอาจถูกตอบกลับเป็นภาษาอังกฤษได้ ดังนั้น เราจึงทำการแก้ไขปัญหาด้วยการบังคับโมเดลให้รันเฉฉพาะภาษาไทยเท่านั้น คุณสามารถปิดการตั้งค่านี้ได้ตามช่่อง \"กำหนดภาษาสำหรับการตอบกลับ\""
      ],
      "metadata": {
        "id": "MIlnBZ7O4aId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### หน้า ChatBot\n",
        "\n",
        "![](https://i.imgur.com/rZiG1HA.png)\n",
        "\n",
        "แท็บนี้เป็นหน้าต่างให้สนทนากับ AI ผ่านรูปแบบแชท ให้คุณพิมพ์ข้อความในช่องข้อความ \"พิมพ์คำถามของคุณที่นี่... (กด enter หรือ submit หลังพิมพ์เสร็จ)\" แล้วกด enter หรือ submit เพื่อให้ AI ตอบแชทของคุณ\n",
        "\n",
        "สำหรับ Feedback คุณสามารถส่ง Feedback การตอบแชทของ AI ได้ โดยมี 3 ช่อง\n",
        "\n",
        "1. Good - คำตอบดี ตรงกับคำถาม\n",
        "2. Bad - แย่ คำตอบไม่ตรง ตอบผิด\n",
        "3. Report - คำตอบออกมาไม่เหมาะสม หรือ ผิดพลาดอย่างร้ายแรง\n",
        "\n",
        "และคุณสามารถเสนอแนะเพิ่มเติมในช่อง Feedback chatbot แล้วกด Submit Feedback เพื่อส่ง Feedback ของคุณ ถ้าสำเร็จจะขึ้น Thank you for feedback และช่องข้อความจะหายไป"
      ],
      "metadata": {
        "id": "mI4O56Vi4Ij9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### หน้า Text Generation\n",
        "\n",
        "![](https://i.imgur.com/8uC2zbP.png)\n",
        "\n",
        "เป็นหน้าสำหรับให้สร้างข้อความปรับแต่งค่าได้ โดยให้ใส่ค่าต่อไปนี้\n",
        "\n",
        "- Instruction: สำหรับใส่คำสั่งที่ต้องการให้ AI ทำงาน\n",
        "- Context input: สำหรับใส่เนื้อหาของคำสั่ง (ไม่จำเป็น)\n",
        "\n",
        "จากนั้นยังสามารถกำหนดค่า Max new tokens ได้ และสามารถปรับแต่ง Advanced options ได้ ทั้งค่า Temperature และ Top p (ไม่จำเป็น)\n",
        "\n",
        "หลังจากใส่ครบแล้ว สามารถกด Run ได้เลย ผลลัพธ์จะขึ้นที่ช่องทางขวามือ และมีช่อง Feedback\n",
        "\n",
        "สำหรับ Feedback คุณสามารถส่ง Feedback การตอบแชทของ AI ได้ โดยมี 3 ช่อง\n",
        "\n",
        "1. Good - คำตอบดี ตรงกับคำถาม\n",
        "2. Bad - แย่ คำตอบไม่ตรง ตอบผิด\n",
        "3. Report - คำตอบออกมาไม่เหมาะสม หรือ ผิดพลาดอย่างร้ายแรง\n",
        "\n",
        "และคุณสามารถเสนอแนะเพิ่มเติมในช่อง Feedback chatbot แล้วกด Submit Feedback เพื่อส่ง Feedback ของคุณ ถ้าสำเร็จจะขึ้น Thank you for feedback และช่องข้อความจะหายไป"
      ],
      "metadata": {
        "id": "BPZ3E67h4OfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model"
      ],
      "metadata": {
        "id": "YU6FwftGeOMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title กำหนดภาษาสำหรับการตอบกลับ\n",
        "#@markdown คุณต้องการให้ AI ตอบกลับเป็นภาษาไทยเท่านั้นหรือไม่? (Yes คือ ใช่ และ No คือ ไม่)\n",
        "Thai = \"Yes\" #@param [\"Yes\", \"No\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "utgm6ucFqhTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "EmFKjc0x3zWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run\n",
        "#@markdown วิธีการใช้งาน สำหรับแชทให้คลิกแท็บ chatbot ส่วนหากต้องการสร้างข้อความให้ไปที่ Text Generation\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain import ConversationChain, LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import torch\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "\n",
        "template = \"\"\"\n",
        "{history}\n",
        "<human>: {human_input}\n",
        "<bot>:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"], \n",
        "    template=template\n",
        ")\n",
        "exclude_pattern = re.compile(r'[^ก-๙]+') #|[^0-9a-zA-Z]+\n",
        "def is_exclude(text):\n",
        "   return bool(exclude_pattern.search(text))\n",
        "\n",
        "df = pd.DataFrame(tokenizer.vocab.items(), columns=['text', 'idx'])\n",
        "df['is_exclude'] = df.text.map(is_exclude)\n",
        "exclude_ids = df[df.is_exclude==True].idx.tolist()\n",
        "if Thai==\"Yes\":\n",
        "  pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    begin_suppress_tokens=exclude_ids,\n",
        "    no_repeat_ngram_size=2,\n",
        "  )\n",
        "else:\n",
        "  pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    no_repeat_ngram_size=2,\n",
        "  )\n",
        "hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=hf_pipeline, \n",
        "    prompt=prompt, \n",
        "    verbose=True, \n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        ")\n",
        "\n",
        "\n",
        "api_url = \"https://wangchanglm.numfa.com/api.php\" # Don't open this url!!!\n",
        "import requests\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import urlencode\n",
        "from urllib.error import HTTPError, URLError\n",
        "from urllib.request import Request\n",
        "import copy\n",
        "\n",
        "def sumbit_data(save,prompt,vote,feedback=None,max_len=None,temp=None,top_p=None,name_model=name_model):\n",
        "  api_url = \"https://wangchanglm.numfa.com/api.php\" \n",
        "  myobj = {\n",
        "      'save': save,\n",
        "      'prompt':prompt,\n",
        "      'vote':vote,\n",
        "      'feedback':feedback,\n",
        "      'max_len':max_len,\n",
        "      'temp':temp,\n",
        "      'top_p':top_p,\n",
        "      'model':name_model\n",
        "  }\n",
        "  _temp_url =\"https://wangchanglm.numfa.com/api.php\" \n",
        "  _temp_url += \"?\" + urlencode(myobj, doseq=True, safe=\"/\")\n",
        "  html = urlopen(_temp_url).read().decode('utf-8')\n",
        "  return True\n",
        "\n",
        "\n",
        "def gen_instruct(text,max_new_tokens=512,top_p=0.95,temperature=0.9,top_k=50):\n",
        "    batch = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.cuda.amp.autocast(): # cuda -> cpu if cpu\n",
        "        if Thai==\"Yes\":\n",
        "          output_tokens = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens, # 512\n",
        "            begin_suppress_tokens = exclude_ids,\n",
        "            no_repeat_ngram_size=2,\n",
        "            #oasst k50\n",
        "            top_k=top_k,\n",
        "            top_p=top_p, # 0.95\n",
        "            typical_p=1.,\n",
        "            temperature=temperature, # 0.9\n",
        "          )\n",
        "        else:\n",
        "          output_tokens = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens, # 512\n",
        "            no_repeat_ngram_size=2,\n",
        "            #oasst k50\n",
        "            top_k=top_k,\n",
        "            top_p=top_p, # 0.95\n",
        "            typical_p=1.,\n",
        "            temperature=temperature, # 0.9\n",
        "          )\n",
        "    return tokenizer.decode(output_tokens[0][len(batch[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "\n",
        "def gen_chatbot_old(text):\n",
        "    batch = tokenizer(text, return_tensors=\"pt\")\n",
        "    #context_tokens = tokenizer(text, add_special_tokens=False)['input_ids']\n",
        "    #logits_processor = FocusContextProcessor(context_tokens, model.config.vocab_size, scaling_factor = 1.5)\n",
        "    with torch.cpu.amp.autocast(): # cuda if gpu\n",
        "        output_tokens = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            max_new_tokens=512,\n",
        "            begin_suppress_tokens = exclude_ids,\n",
        "            no_repeat_ngram_size=2,\n",
        "        )\n",
        "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True).split(\": \")[-1]\n",
        "\n",
        "def list2prompt(history):\n",
        "    _text = \"\"\n",
        "    for user,bot in history:\n",
        "        _text+=\"<human>: \"+user+\"\\n<bot>: \"\n",
        "        if bot!=None:\n",
        "            _text+=bot+\"\\n\"\n",
        "    return _text\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"<context>: {input}\\n<human>: {instruction}\\n<bot>: \"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"<human>: {instruction}\\n<bot>: \"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def instruct_generate(\n",
        "    instruct: str,\n",
        "    input: str = 'none',\n",
        "    max_gen_len=512,\n",
        "    temperature: float = 0.1,\n",
        "    top_p: float = 0.75,\n",
        "):\n",
        "    if input == 'none' or len(input)<2:\n",
        "        prompt = PROMPT_DICT['prompt_no_input'].format_map(\n",
        "            {'instruction': instruct, 'input': ''})\n",
        "    else:\n",
        "        prompt = PROMPT_DICT['prompt_input'].format_map(\n",
        "            {'instruction': instruct, 'input': input})\n",
        "    result = gen_instruct(prompt,max_gen_len,top_p,temperature)\n",
        "    return result\n",
        "\n",
        "with gr.Blocks(height=900) as demo:\n",
        "    chatgpt_chain = LLMChain(\n",
        "        llm=hf_pipeline, \n",
        "        prompt=prompt, \n",
        "        verbose=True, \n",
        "        memory=ConversationBufferWindowMemory(k=2),\n",
        "    )\n",
        "    with gr.Tab(\"Text Generation\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                instruction = gr.Textbox(lines=2, label=\"Instruction\",max_lines=10)\n",
        "                input = gr.Textbox(\n",
        "                    lines=2, label=\"Context input\", placeholder='none',max_lines=5)\n",
        "                max_len = gr.Slider(minimum=1, maximum=1024,\n",
        "                                    value=512, label=\"Max new tokens\")\n",
        "                with gr.Accordion(label='Advanced options', open=False):\n",
        "                    temp = gr.Slider(minimum=0, maximum=1,\n",
        "                                     value=0.9, label=\"Temperature\")\n",
        "                    top_p = gr.Slider(minimum=0, maximum=1,\n",
        "                                      value=0.95, label=\"Top p\")\n",
        "\n",
        "                run_botton = gr.Button(\"Run\")\n",
        "\n",
        "            with gr.Column():\n",
        "                outputs = gr.Textbox(lines=10, label=\"Output\")\n",
        "                with gr.Column(visible=False) as feedback_gen_box:\n",
        "                    gen_radio = gr.Radio(\n",
        "                        [\"Good\", \"Bad\", \"Report\"], label=\"Do you think about the chat?\")\n",
        "                    feedback_gen = gr.Textbox(placeholder=\"Feedback chatbot\",show_label=False, lines=4)\n",
        "                    feedback_gen_submit = gr.Button(\"Submit Feedback\")\n",
        "                with gr.Row(visible=False) as feedback_gen_ok:\n",
        "                    gr.Markdown(\"Thank you for feedback.\")\n",
        "\n",
        "        def save_up2(instruction, input,prompt,max_len,temp,top_p,choice,feedback):\n",
        "            save=\"gen\"\n",
        "            if input == 'none' or len(input)<2:\n",
        "              _prompt = PROMPT_DICT['prompt_no_input'].format_map(\n",
        "                  {'instruction': instruction, 'input': ''})\n",
        "            else:\n",
        "              _prompt = PROMPT_DICT['prompt_input'].format_map(\n",
        "                  {'instruction': instruction, 'input': input})\n",
        "            prompt=_prompt+prompt\n",
        "            if choice==\"Good\":\n",
        "              sumbit_data(save=save,prompt=prompt,vote=1,feedback=feedback,max_len=max_len,temp=temp,top_p=top_p)\n",
        "            elif choice==\"Bad\":\n",
        "              sumbit_data(save=save,prompt=prompt,vote=0,feedback=feedback,max_len=max_len,temp=temp,top_p=top_p)\n",
        "            else:\n",
        "              sumbit_data(save=save,prompt=prompt,vote=3,feedback=feedback,max_len=max_len,temp=temp,top_p=top_p)\n",
        "            return {feedback_gen_box: gr.update(visible=False),feedback_gen_ok: gr.update(visible=True)}\n",
        "        def gen(instruct: str,input: str = 'none',max_gen_len=512,temperature: float = 0.1,top_p: float = 0.75):\n",
        "            feedback_gen_ok.update(visible=False)\n",
        "            _temp= instruct_generate(instruct,input,max_gen_len,temperature,top_p)\n",
        "            feedback_gen_box.update(visible=True)\n",
        "            return {outputs:_temp,feedback_gen_box: gr.update(visible=True),feedback_gen_ok: gr.update(visible=False)}\n",
        "        feedback_gen_submit.click(fn=save_up2, inputs=[instruction, input,outputs,max_len,temp,top_p,gen_radio,feedback_gen], outputs=[feedback_gen_box,feedback_gen_ok], queue=True)\n",
        "        inputs = [instruction, input, max_len, temp, top_p]\n",
        "        run_botton.click(fn=gen, inputs=inputs, outputs=[outputs,feedback_gen_box,feedback_gen_ok])\n",
        "        examples = gr.Examples(examples=[(\"แต่งกลอนวันแม่\"), (\"แต่งกลอนแปดวันแม่\")],inputs=[instruction])\n",
        "    with gr.Tab(\"ChatBot\"):\n",
        "        with gr.Column():\n",
        "            chatbot = gr.Chatbot(label=\"Chat Message Box\", placeholder=\"Chat Message Box\",show_label=False).style(container=False)\n",
        "        with gr.Row():\n",
        "          with gr.Column(scale=0.85):\n",
        "            msg = gr.Textbox(placeholder=\"พิมพ์คำถามของคุณที่นี่... (กด enter หรือ submit หลังพิมพ์เสร็จ)\",show_label=False)\n",
        "          with gr.Column(scale=0.15, min_width=0):\n",
        "            submit = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            with gr.Column(visible=False) as feedback_chatbot_box:\n",
        "                chatbot_radio = gr.Radio(\n",
        "                    [\"Good\", \"Bad\", \"Report\"], label=\"Do you think about the chat?\"\n",
        "                )\n",
        "                feedback_chatbot = gr.Textbox(placeholder=\"Feedback chatbot\",show_label=False, lines=4)\n",
        "                feedback_chatbot_submit = gr.Button(\"Submit Feedback\")\n",
        "            with gr.Row(visible=False) as feedback_chatbot_ok:\n",
        "                gr.Markdown(\"Thank you for feedback.\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "        def save_up(history,choice,feedback):\n",
        "            _bot = list2prompt(history)\n",
        "            x=False\n",
        "            if choice==\"Good\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=1,feedback=feedback)\n",
        "            elif choice==\"Bad\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=0,feedback=feedback)\n",
        "            else:\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=3,feedback=feedback)\n",
        "            return {feedback_chatbot_ok: gr.update(visible=True),feedback_chatbot_box: gr.update(visible=False)}\n",
        "        def user(user_message, history):\n",
        "            bot_message = chatgpt_chain.predict(human_input=user_message)\n",
        "            history.append((user_message, bot_message))\n",
        "            return \"\", history,gr.update(visible=True)\n",
        "        def reset():\n",
        "          chatgpt_chain.memory.clear()\n",
        "          print(\"clear!\")\n",
        "        feedback_chatbot_submit.click(fn=save_up, inputs=[chatbot,chatbot_radio,feedback_chatbot], outputs=[feedback_chatbot_ok,feedback_chatbot_box,], queue=True)\n",
        "        clear.click(reset, None, chatbot, queue=False)\n",
        "        submit_event = msg.submit(fn=user, inputs=[msg, chatbot], outputs=[msg, chatbot,feedback_chatbot_box], queue=True)\n",
        "        submit_click_event = submit.click(fn=user, inputs=[msg, chatbot], outputs=[msg, chatbot,feedback_chatbot_box], queue=True)\n",
        "    with gr.Tab(\"ChatBot without LangChain\"):\n",
        "        chatbot2 = gr.Chatbot()\n",
        "        msg2 = gr.Textbox(label=\"Your sentence here... (press enter to submit)\")\n",
        "        with gr.Column():\n",
        "            with gr.Column(visible=False) as feedback_chatbot_box2:\n",
        "                chatbot_radio2 = gr.Radio(\n",
        "                    [\"Good\", \"Bad\", \"Report\"], label=\"Do you think about the chat?\"\n",
        "                )\n",
        "                feedback_chatbot2 = gr.Textbox(placeholder=\"Feedback chatbot\",show_label=False, lines=4)\n",
        "                feedback_chatbot_submit2 = gr.Button(\"Submit Feedback\")\n",
        "            with gr.Row(visible=False) as feedback_chatbot_ok2:\n",
        "                gr.Markdown(\"Thank you for feedback.\")\n",
        "        \n",
        "        def user2(user_message, history):\n",
        "            return \"\", history + [[user_message, None]]\n",
        "        def bot2(history):\n",
        "            _bot = list2prompt(history)\n",
        "            bot_message = gen_chatbot_old(_bot)\n",
        "            history[-1][1] = bot_message\n",
        "            return history,gr.update(visible=True)\n",
        "        def save_up2(history,choice,feedback):\n",
        "            _bot = list2prompt(history)\n",
        "            x=False\n",
        "            if choice==\"Good\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=1,feedback=feedback,name_model=name_model+\"-chat_old\")\n",
        "            elif choice==\"Bad\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=0,feedback=feedback,name_model=name_model+\"-chat_old\")\n",
        "            else:\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=3,feedback=feedback,name_model=name_model+\"-chat_old\")\n",
        "            return {feedback_chatbot_ok2: gr.update(visible=True),feedback_chatbot_box2: gr.update(visible=False)}\n",
        "        msg2.submit(user2, [msg2, chatbot2], [msg2, chatbot2], queue=False).then(bot2, chatbot2, [chatbot2,feedback_chatbot_box2])\n",
        "        feedback_chatbot_submit2.click(fn=save_up2, inputs=[chatbot2,chatbot_radio2,feedback_chatbot2], outputs=[feedback_chatbot_ok2,feedback_chatbot_box2], queue=True)\n",
        "        clear2 = gr.Button(\"Clear\")\n",
        "        clear2.click(lambda: None, None, chatbot2, queue=False)\n",
        "demo.queue()\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "7lHnjEOU-UNB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ตัวอย่างเพิ่มเติม\n",
        "\n",
        "- [🦜⛓️ LangChain + 🐘 WangchanGLM](https://colab.research.google.com/drive/1pEZdM8lUVBIvgInbG2K7O-IdCX5fLwy3#scrollTo=GG_A9G4kHUBM)\n",
        "\n",
        "ขอความร่วมมือท่านช่วยส่ง feedback การใช้งานกลับมา เพื่อให้เราพัฒนาโมเดลรุ่นหน้าออกมาที่ดีกว่าเดิมให้ใช้งานฟรี"
      ],
      "metadata": {
        "id": "PdZG5m3gXICK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "คุณสามารถติดตามข่าวสารเกี่ยวกับ 🐘 WangChanGLM ได้ผ่านทาง Facebook: [AIResearch.in.th](https://www.facebook.com/AIResearch.in.th) กับ [PyThaiNLP](https://www.facebook.com/pythainlp)"
      ],
      "metadata": {
        "id": "u4gkgn7p5XgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ข่าว**\n",
        "\n",
        "เตรียมตัวมาพบกันพฤษภาคมนี้!\n",
        "\n",
        "VISTEC AI Day 2023\n",
        "วันที่ 22 พฤษภาคม 13:30-16:30 น.\n",
        "The Synergy Hall, ชั้น 6, Energy Complex, Building C\n",
        "พบกับเทคโนโลยีปัญญาประดิษฐ์ชั้นแนวหน้าจาก VISTEC ครั้งใหญ่\n",
        "ที่จะแสดงศักยภาพจากงานวิจัยเชิงลึกสู่ประโยชน์เชิงธุรกิจ\n",
        "พบกับบริษัท หน่วยวิจัย และงานวิจัยทางด้านปัญญาประดิษฐ์สุดล้ำ:\n",
        "- VISAI.aiผู้ให้บริการ AI ครบวงจรสำหรับภาคธุรกิจ\n",
        "Wangchan Advance Industrial Labs: แล็บวิจัยพัฒนาเทคโนโลยี ปัญญาประดิษฐ์ หุ่นยนต์ และเซนเซอร์\n",
        "- WangchanX แนะนำโปรแกรมวิจัยพัฒนาโมเดลภาษาขนาดใหญ่ ที่มียอดดาวน์โหลดกว่า 1.2 ล้านครั้ง และการเปิดตัว WangChanGLM โมเดลภาษาเชิง generative ที่รองรับภาษาไทย และภาษาอื่น ๆ หลากหลายภาษา\n",
        "- VISTEC-Siriraj Frontier Research Center ศูนย์วิจัยชั้นแนวหน้าสำหรับเทคโนโลยีการแพทย์ในอนาคต \n",
        "- SensAI เทคโนโลยีเซนเซอร์อัจฉริยะสำหรับการแพทย์เชิงป้องกันและการดูแลผู้ป่วยทางไกล\n",
        "มาร่วมพบปะกับผู้เชี่ยวชาญ AI และบุคลากรสายเทค พร้อมเยี่ยมชมบูธได้ฟรี\n",
        "ลงทะเบียนเข้าร่วมงานได้ที่ https://forms.office.com/r/y2nbppaG9g \n",
        "\n",
        "ติดต่อสอบถามเพิ่มเติมได้ที่ wachirawish_pro@vistec.ac.th\n"
      ],
      "metadata": {
        "id": "gPLfnj8l5967"
      }
    }
  ]
}