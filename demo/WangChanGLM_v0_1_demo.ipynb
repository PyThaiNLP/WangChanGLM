{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üêò WangChanGLM v0.2 demo\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pythainlp/WangChanGLM/blob/main/demo/WangChanGLM_v0_1_demo.ipynb)\n",
        "\n",
        "- Notebook ‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• WangchanGPT ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (large language model) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å fine-tune ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà text prompt ‡πÑ‡∏î‡πâ\n",
        "- ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å [Medium blog](https://medium.com/@iwishcognitivedissonance/7aa9a0f51f5f)\n",
        "- Notebook ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ license [Apache Software License 2.0](https://github.com/PyThaiNLP/pythainlp/blob/dev/LICENSE).\n",
        "- Repository: [pythainlp/wangchanglm](https://github.com/PyThaiNLP/WangChanGLM)\n",
        "- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏≤‡∏£‡∏£‡∏±‡∏ô ‡πÉ‡∏´‡πâ login ‡∏ö‡∏±‡∏ç‡∏ä‡∏µ Google ‡πÅ‡∏•‡πâ‡∏ß\n",
        "  - ‡∏Å‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ö Runtime ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏•‡∏¥‡∏Å Run all\n",
        "  - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ö ‡∏£‡∏±‡∏ô‡πÑ‡∏ó‡∏°‡πå ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏•‡∏¥‡πâ‡∏Å ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå\n",
        "  - ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ colab gpu ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô notebook ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏±‡∏Ñ‡∏£ colab pro\n",
        "\n",
        "‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤: [PyThaiNLP](https://github.com/PyThaiNLP) ‡πÅ‡∏•‡∏∞ [VISTEC-depa AI Research Thailand](https://airesearch.in.th/)\n",
        "\n",
        "![](https://avatars.githubusercontent.com/u/32934255?s=45&v=4)\n",
        "![](https://airesearch.in.th/assets/img/logo/airesearch-logo.svg)\n",
        "\n",
        "**‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö**"
      ],
      "metadata": {
        "id": "Ixo7mcoAHiSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install\n",
        "!pip install --q torch sentencepiece accelerate transformers pandas requests gradio bitsandbytes langchain faiss-gpu sentence-transformers gradio\n",
        "!pip3 install tensorflow_text>=2.0.0rc0"
      ],
      "metadata": {
        "id": "RI6wPaY5a1Rl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Scripts\n",
        "\n",
        "!wget -O sensitive_topics.pkl https://github.com/PyThaiNLP/WangChanGLM/raw/main/static/sensitive_topics_v2.pkl\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "\n",
        "\n",
        "class Encoder(ABC):\n",
        "    @abstractmethod\n",
        "    def encode(self, texts: List[str]) -> np.array:\n",
        "      \"\"\"\n",
        "        output dimension expected to be one dimension and normalized (unit vector)\n",
        "      \"\"\"\n",
        "      ...\n",
        "\n",
        "\n",
        "class MUSEEncoder(Encoder):\n",
        "    def __init__(self, model_url: str = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"):\n",
        "        self.embed = hub.load(model_url)\n",
        "\n",
        "    def encode(self, texts: List[str]) -> np.array:\n",
        "        embeds = self.embed(texts).numpy()\n",
        "        embeds = embeds / np.linalg.norm(embeds, axis=1).reshape(embeds.shape[0], -1)\n",
        "        return embeds\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SensitiveTopic:\n",
        "    name: str\n",
        "    respond_message: str\n",
        "    sensitivity: float = None # range from 0 to 1\n",
        "    demonstrations: List[str] = None\n",
        "    adhoc_embeded_demonstrations: np.array = None # dimension = [N_ADHOC, DIM]. Please kindly note that this suppose to \n",
        "\n",
        "\n",
        "DEFAULT_SENSITIVITY = 0.7\n",
        "\n",
        "\n",
        "class SensitiveTopicProtector:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sensitive_topics: List[SensitiveTopic],\n",
        "        encoder: Encoder = MUSEEncoder(),\n",
        "        default_sensitivity: float = DEFAULT_SENSITIVITY\n",
        "    ):\n",
        "        self.sensitive_topics = sensitive_topics\n",
        "        self.default_sensitivity = default_sensitivity\n",
        "        self.encoder = encoder\n",
        "        self.topic_embeddings = self._get_topic_embeddings()\n",
        "\n",
        "    def _get_topic_embeddings(self) -> Dict[str, List[np.array]]:\n",
        "        topic_embeddings = {}\n",
        "        for topic in self.sensitive_topics:\n",
        "            current_topic_embeddings = None\n",
        "            if topic.demonstrations is not None:\n",
        "                current_topic_embeddings = self.encoder.encode(texts=topic.demonstrations) if current_topic_embeddings is None \\\n",
        "                    else np.concatenate((current_topic_embeddings, self.encoder.encode(texts=topic.demonstrations)), axis=0)\n",
        "            if topic.adhoc_embeded_demonstrations is not None:\n",
        "                current_topic_embeddings = topic.adhoc_embeded_demonstrations if current_topic_embeddings is None \\\n",
        "                    else np.concatenate((current_topic_embeddings, topic.adhoc_embeded_demonstrations), axis=0)\n",
        "            topic_embeddings[topic.name] = current_topic_embeddings\n",
        "        return topic_embeddings\n",
        "\n",
        "    def filter(self, text: str) -> Tuple[bool, str]:\n",
        "        is_sensitive, respond_message = False, None\n",
        "        text_embedding = self.encoder.encode([text,])\n",
        "        for topic in self.sensitive_topics:\n",
        "            risk_scores = np.einsum('ik,jk->j', text_embedding, self.topic_embeddings[topic.name])\n",
        "            max_risk_score = np.max(risk_scores)\n",
        "            if topic.sensitivity:\n",
        "                if max_risk_score > (1.0 - topic.sensitivity):\n",
        "                    return True, topic.respond_message\n",
        "                continue\n",
        "            if max_risk_score > (1.0 - self.default_sensitivity):\n",
        "                return True, topic.respond_message\n",
        "        return is_sensitive, respond_message\n",
        "    \n",
        "    @classmethod\n",
        "    def fromRaw(cls, raw_sensitive_topics: List[Dict], encoder: Encoder = MUSEEncoder(), default_sensitivity: float = DEFAULT_SENSITIVITY):\n",
        "        sensitive_topics = [SensitiveTopic(**topic) for topic in raw_sensitive_topics]\n",
        "        return cls(sensitive_topics=sensitive_topics, encoder=encoder, default_sensitivity=default_sensitivity)\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "f = open(\"sensitive_topics.pkl\", \"rb\")\n",
        "sensitive_topics = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "guardian = SensitiveTopicProtector.fromRaw(sensitive_topics)\n",
        "### Test Cases\n",
        "is_sensitive, respond_message = guardian.filter(\"‡∏´‡∏∏‡πâ‡πà‡∏ô‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\")\n",
        "assert is_sensitive == True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cb4cJwsv0Hbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "#@markdown WangChanGLM v0.1 ‡∏°‡∏µ 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Ñ‡∏∑‡∏≠\n",
        "#@markdown \n",
        "#@markdown 1. pythainlp/wangchanglm-7.5B-sft-en-sharded -> ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)\n",
        "#@markdown 2. pythainlp/wangchanglm-7.5B-sft-enth-sharded -> ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ì‡∏µ\n",
        "#@markdown\n",
        "#@markdown **‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô** ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏•‡∏∑‡πÅ‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏î Runtime -> Disconnect Colab Runtime ‡πÅ‡∏•‡πâ‡∏ß Run all ‡∏´‡∏£‡∏∑‡∏≠ ‡∏£‡∏±‡∏ô‡πÑ‡∏ó‡∏°‡πå -> ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏£‡∏±‡∏ô‡πÑ‡∏ó‡∏°‡πå ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏•‡∏¥‡∏Å ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå\n",
        "name_model = \"pythainlp/wangchanglm-7.5B-sft-en-sharded\" #@param [\"pythainlp/wangchanglm-7.5B-sft-en-sharded\",\"pythainlp/wangchanglm-7.5B-sft-enth-sharded\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jpPRZKjxuEgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrKHUyL0WNjX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download Model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import Optional, Union, List, Dict, Any\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# name_model = \"pythainlp/wangchanglm-7.5B-sft-adapter-merged-sharded\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    name_model, \n",
        "    return_dict=True, \n",
        "    load_in_8bit=True , # try use load_in_8bit=False for use fp16 :D\n",
        "    device_map=\"auto\", \n",
        "    torch_dtype=torch.float16, \n",
        "    offload_folder=\"./\", \n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-7.5B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô"
      ],
      "metadata": {
        "id": "Sritb3XEwMpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏†‡∏≤‡∏©‡∏≤\n",
        "\n",
        "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å WangchanGPT ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Multilingual Instruction-Following Model ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÑ‡∏î‡πâ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÄ‡∏â‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ä‡πà‡πà‡∏≠‡∏á \"‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏†‡∏≤‡∏©‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö\""
      ],
      "metadata": {
        "id": "MIlnBZ7O4aId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‡∏´‡∏ô‡πâ‡∏≤ ChatBot\n",
        "\n",
        "![](https://i.imgur.com/rZiG1HA.png)\n",
        "\n",
        "‡πÅ‡∏ó‡πá‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡∏±‡∏ö AI ‡∏ú‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° \"‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà... (‡∏Å‡∏î enter ‡∏´‡∏£‡∏∑‡∏≠ submit ‡∏´‡∏•‡∏±‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏™‡∏£‡πá‡∏à)\" ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î enter ‡∏´‡∏£‡∏∑‡∏≠ submit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ä‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "\n",
        "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Feedback ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á Feedback ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÅ‡∏ä‡∏ó‡∏Ç‡∏≠‡∏á AI ‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ 3 ‡∏ä‡πà‡∏≠‡∏á\n",
        "\n",
        "1. Good - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡∏µ ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
        "2. Bad - ‡πÅ‡∏¢‡πà ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á ‡∏ï‡∏≠‡∏ö‡∏ú‡∏¥‡∏î\n",
        "3. Report - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á\n",
        "\n",
        "‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á Feedback chatbot ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Submit Feedback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á Feedback ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏ñ‡πâ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô Thank you for feedback ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡∏´‡∏≤‡∏¢‡πÑ‡∏õ"
      ],
      "metadata": {
        "id": "mI4O56Vi4Ij9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‡∏´‡∏ô‡πâ‡∏≤ Text Generation\n",
        "\n",
        "![](https://i.imgur.com/8uC2zbP.png)\n",
        "\n",
        "‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n",
        "\n",
        "- Instruction: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
        "- Context input: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "\n",
        "‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Max new tokens ‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Advanced options ‡πÑ‡∏î‡πâ ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Temperature ‡πÅ‡∏•‡∏∞ Top p (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "\n",
        "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏™‡πà‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏î Run ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡∏°‡∏∑‡∏≠ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á Feedback\n",
        "\n",
        "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Feedback ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á Feedback ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÅ‡∏ä‡∏ó‡∏Ç‡∏≠‡∏á AI ‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ 3 ‡∏ä‡πà‡∏≠‡∏á\n",
        "\n",
        "1. Good - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡∏µ ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
        "2. Bad - ‡πÅ‡∏¢‡πà ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á ‡∏ï‡∏≠‡∏ö‡∏ú‡∏¥‡∏î\n",
        "3. Report - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á\n",
        "\n",
        "‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á Feedback chatbot ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Submit Feedback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á Feedback ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏ñ‡πâ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô Thank you for feedback ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡∏´‡∏≤‡∏¢‡πÑ‡∏õ"
      ],
      "metadata": {
        "id": "BPZ3E67h4OfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model"
      ],
      "metadata": {
        "id": "YU6FwftGeOMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏†‡∏≤‡∏©‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö\n",
        "#@markdown ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (Yes ‡∏Ñ‡∏∑‡∏≠ ‡πÉ‡∏ä‡πà ‡πÅ‡∏•‡∏∞ No ‡∏Ñ‡∏∑‡∏≠ ‡πÑ‡∏°‡πà)\n",
        "Thai = \"Yes\" #@param [\"Yes\", \"No\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "utgm6ucFqhTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "EmFKjc0x3zWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run\n",
        "#@markdown ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏ó‡πá‡∏ö chatbot ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Text Generation\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain import ConversationChain, LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import torch\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "\n",
        "template = \"\"\"\n",
        "{history}\n",
        "<human>: {human_input}\n",
        "<bot>:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"], \n",
        "    template=template\n",
        ")\n",
        "exclude_pattern = re.compile(r'[^‡∏Å-‡πô]+') #|[^0-9a-zA-Z]+\n",
        "def is_exclude(text):\n",
        "   return bool(exclude_pattern.search(text))\n",
        "\n",
        "df = pd.DataFrame(tokenizer.vocab.items(), columns=['text', 'idx'])\n",
        "df['is_exclude'] = df.text.map(is_exclude)\n",
        "exclude_ids = df[df.is_exclude==True].idx.tolist()\n",
        "if Thai==\"Yes\":\n",
        "  pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    begin_suppress_tokens=exclude_ids,\n",
        "    no_repeat_ngram_size=2,\n",
        "  )\n",
        "else:\n",
        "  pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    no_repeat_ngram_size=2,\n",
        "  )\n",
        "hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=hf_pipeline, \n",
        "    prompt=prompt, \n",
        "    verbose=True, \n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        ")\n",
        "\n",
        "\n",
        "api_url = \"https://wangchanglm.numfa.com/apiv2.php\" # Don't open this url!!!\n",
        "import requests\n",
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import urlencode\n",
        "from urllib.error import HTTPError, URLError\n",
        "from urllib.request import Request\n",
        "import copy\n",
        "\n",
        "def sumbit_data(save,prompt,vote,feedback=None,max_len=None,temp=None,top_p=None,name_model=name_model):\n",
        "  api_url = \"https://wangchanglm.numfa.com/apiv2.php\" \n",
        "  myobj = {\n",
        "      'save': save,\n",
        "      'prompt':prompt,\n",
        "      'vote':vote,\n",
        "      'feedback':feedback,\n",
        "      'max_len':max_len,\n",
        "      'temp':temp,\n",
        "      'top_p':top_p,\n",
        "      'model':name_model\n",
        "  }\n",
        "  myobj=[(k, v) for k, v in myobj.items()]\n",
        "  myobj=urllib.parse.urlencode(myobj)\n",
        "  utf8 = bytes(myobj, 'utf-8')\n",
        "  #req = urllib.request.Request(api_url)\n",
        "  #req.add_header(\"Content-type\", \"application/x-www-form-urlencoded\")\n",
        "  page=urllib.request.urlopen(api_url, utf8, 300).read()\n",
        "  return True\n",
        "\n",
        "\n",
        "def gen_instruct(text,max_new_tokens=512,top_p=0.95,temperature=0.9,top_k=50):\n",
        "    batch = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.cuda.amp.autocast(): # cuda -> cpu if cpu\n",
        "        if Thai==\"Yes\":\n",
        "          output_tokens = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens, # 512\n",
        "            begin_suppress_tokens = exclude_ids,\n",
        "            no_repeat_ngram_size=2,\n",
        "            #oasst k50\n",
        "            top_k=top_k,\n",
        "            top_p=top_p, # 0.95\n",
        "            typical_p=1.,\n",
        "            temperature=temperature, # 0.9\n",
        "          )\n",
        "        else:\n",
        "          output_tokens = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens, # 512\n",
        "            no_repeat_ngram_size=2,\n",
        "            #oasst k50\n",
        "            top_k=top_k,\n",
        "            top_p=top_p, # 0.95\n",
        "            typical_p=1.,\n",
        "            temperature=temperature, # 0.9\n",
        "          )\n",
        "    return tokenizer.decode(output_tokens[0][len(batch[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "\n",
        "def gen_chatbot_old(text):\n",
        "    is_sensitive, respond_message = guardian.filter(text)\n",
        "    if is_sensitive:\n",
        "        return respond_message\n",
        "\n",
        "    batch = tokenizer(text, return_tensors=\"pt\")\n",
        "    #context_tokens = tokenizer(text, add_special_tokens=False)['input_ids']\n",
        "    #logits_processor = FocusContextProcessor(context_tokens, model.config.vocab_size, scaling_factor = 1.5)\n",
        "    with torch.cpu.amp.autocast(): # cuda if gpu\n",
        "        output_tokens = model.generate(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            max_new_tokens=512,\n",
        "            begin_suppress_tokens = exclude_ids,\n",
        "            no_repeat_ngram_size=2,\n",
        "        )\n",
        "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True).split(\": \")[-1]\n",
        "\n",
        "def list2prompt(history):\n",
        "    _text = \"\"\n",
        "    for user,bot in history:\n",
        "        _text+=\"<human>: \"+user+\"\\n<bot>: \"\n",
        "        if bot!=None:\n",
        "            _text+=bot+\"\\n\"\n",
        "    return _text\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"<context>: {input}\\n<human>: {instruction}\\n<bot>: \"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"<human>: {instruction}\\n<bot>: \"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def instruct_generate(\n",
        "    instruct: str,\n",
        "    input: str = 'none',\n",
        "    max_gen_len=512,\n",
        "    temperature: float = 0.1,\n",
        "    top_p: float = 0.75,\n",
        "):\n",
        "    is_sensitive, respond_message = guardian.filter(instruct)\n",
        "    if is_sensitive:\n",
        "        return respond_message\n",
        "\n",
        "    if input == 'none' or len(input)<2:\n",
        "        prompt = PROMPT_DICT['prompt_no_input'].format_map(\n",
        "            {'instruction': instruct, 'input': ''})\n",
        "    else:\n",
        "        prompt = PROMPT_DICT['prompt_input'].format_map(\n",
        "            {'instruction': instruct, 'input': input})\n",
        "    result = gen_instruct(prompt,max_gen_len,top_p,temperature)\n",
        "    return result\n",
        "\n",
        "with gr.Blocks(height=900) as demo:\n",
        "    chatgpt_chain = LLMChain(\n",
        "        llm=hf_pipeline, \n",
        "        prompt=prompt, \n",
        "        verbose=True, \n",
        "        memory=ConversationBufferWindowMemory(k=2),\n",
        "    )\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    # üêò WangChanGLM v0.2 demo\n",
        "\n",
        "    [Blog](https://medium.com/@iwishcognitivedissonance/wangchanglm-the-thai-turned-multilingual-instruction-following-model-7aa9a0f51f5f) | [Codes](https://github.com/pythainlp/wangchanglm) | [Demo](https://colab.research.google.com/github/pythainlp/WangChanGLM/blob/main/demo/WangChanGLM_v0_1_demo.ipynb)\n",
        "\n",
        "\n",
        "    This demo use CPU only, so It may be slow or very slow. If you want the speed, try [Google colab](https://colab.research.google.com/github/pythainlp/WangChanGLM/blob/main/demo/WangChanGLM_v0_1_demo.ipynb).\n",
        "\n",
        "\n",
        "    **We do not guarantee a reply message.**\n",
        "    \"\"\"\n",
        "    )\n",
        "    with gr.Tab(\"Text Generation\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                instruction = gr.Textbox(lines=2, label=\"Instruction\",max_lines=10)\n",
        "                input = gr.Textbox(\n",
        "                    lines=2, label=\"Context input\", placeholder='none',max_lines=5)\n",
        "                max_len = gr.Slider(minimum=1, maximum=1024,\n",
        "                                    value=512, label=\"Max new tokens\")\n",
        "                with gr.Accordion(label='Advanced options', open=False):\n",
        "                    temp = gr.Slider(minimum=0, maximum=1,\n",
        "                                     value=0.9, label=\"Temperature\")\n",
        "                    top_p = gr.Slider(minimum=0, maximum=1,\n",
        "                                      value=0.95, label=\"Top p\")\n",
        "\n",
        "                run_botton = gr.Button(\"Run\")\n",
        "\n",
        "            with gr.Column():\n",
        "                outputs = gr.Textbox(lines=10, label=\"Output\")\n",
        "                with gr.Column(visible=False) as feedback_gen_box:\n",
        "                    gen_radio = gr.Radio(\n",
        "                        [\"Good\", \"Bad\", \"Report\"], label=\"Do you think about the chat?\")\n",
        "                    feedback_gen = gr.Textbox(placeholder=\"Feedback chatbot\",show_label=False, lines=4)\n",
        "                    feedback_gen_submit = gr.Button(\"Submit Feedback\")\n",
        "                with gr.Row(visible=False) as feedback_gen_ok:\n",
        "                    gr.Markdown(\"Thank you for feedback.\")\n",
        "\n",
        "        def save_up2(instruction, input,prompt,max_len,temp,top_p,choice,feedback):\n",
        "            save=\"gen\"\n",
        "            if input == 'none' or len(input)<2:\n",
        "              _prompt = PROMPT_DICT['prompt_no_input'].format_map(\n",
        "                  {'instruction': instruction, 'input': ''})\n",
        "            else:\n",
        "              _prompt = PROMPT_DICT['prompt_input'].format_map(\n",
        "                  {'instruction': instruction, 'input': input})\n",
        "            prompt=_prompt+prompt\n",
        "            if choice==\"Good\":\n",
        "              sumbit_data(save=save,prompt=prompt,vote=1,feedback=feedback,max_len=max_len,temp=temp,top_p=top_p)\n",
        "            elif choice==\"Bad\":\n",
        "              sumbit_data(save=save,prompt=prompt,vote=0,feedback=feedback,max_len=max_len,temp=temp,top_p=top_p)\n",
        "            else:\n",
        "              sumbit_data(save=save,prompt=prompt,vote=3,feedback=feedback,max_len=max_len,temp=temp,top_p=top_p)\n",
        "            return {feedback_gen_box: gr.update(visible=False),feedback_gen_ok: gr.update(visible=True)}\n",
        "        def gen(instruct: str,input: str = 'none',max_gen_len=512,temperature: float = 0.1,top_p: float = 0.75):\n",
        "            feedback_gen_ok.update(visible=False)\n",
        "            _temp= instruct_generate(instruct,input,max_gen_len,temperature,top_p)\n",
        "            feedback_gen_box.update(visible=True)\n",
        "            return {outputs:_temp,feedback_gen_box: gr.update(visible=True),feedback_gen_ok: gr.update(visible=False)}\n",
        "        feedback_gen_submit.click(fn=save_up2, inputs=[instruction, input,outputs,max_len,temp,top_p,gen_radio,feedback_gen], outputs=[feedback_gen_box,feedback_gen_ok], queue=False)\n",
        "        inputs = [instruction, input, max_len, temp, top_p]\n",
        "        run_botton.click(fn=gen, inputs=inputs, outputs=[outputs,feedback_gen_box,feedback_gen_ok])\n",
        "        examples = gr.Examples(examples=[\"‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏•‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà\",\"‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏•‡∏≠‡∏ô‡πÅ‡∏õ‡∏î‡∏ß‡∏±‡∏ô‡πÅ‡∏°‡πà\",'‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡πâ‡∏ß‡∏ô‡∏ó‡∏≥‡πÑ‡∏á','‡∏à‡∏á‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ù‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢'],inputs=[instruction])\n",
        "    with gr.Tab(\"ChatBot\"):\n",
        "        with gr.Column():\n",
        "            chatbot = gr.Chatbot(label=\"Chat Message Box\", placeholder=\"Chat Message Box\",show_label=False).style(container=False)\n",
        "        with gr.Row():\n",
        "          with gr.Column(scale=0.85):\n",
        "            msg = gr.Textbox(placeholder=\"‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà... (‡∏Å‡∏î enter ‡∏´‡∏£‡∏∑‡∏≠ submit ‡∏´‡∏•‡∏±‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏™‡∏£‡πá‡∏à)\",show_label=False)\n",
        "          with gr.Column(scale=0.15, min_width=0):\n",
        "            submit = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            with gr.Column(visible=False) as feedback_chatbot_box:\n",
        "                chatbot_radio = gr.Radio(\n",
        "                    [\"Good\", \"Bad\", \"Report\"], label=\"Do you think about the chat?\"\n",
        "                )\n",
        "                feedback_chatbot = gr.Textbox(placeholder=\"Feedback chatbot\",show_label=False, lines=4)\n",
        "                feedback_chatbot_submit = gr.Button(\"Submit Feedback\")\n",
        "            with gr.Row(visible=False) as feedback_chatbot_ok:\n",
        "                gr.Markdown(\"Thank you for feedback.\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "        def save_up(history,choice,feedback):\n",
        "            _bot = list2prompt(history)\n",
        "            x=False\n",
        "            if choice==\"Good\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=1,feedback=feedback)\n",
        "            elif choice==\"Bad\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=0,feedback=feedback)\n",
        "            else:\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=3,feedback=feedback)\n",
        "            return {feedback_chatbot_ok: gr.update(visible=True),feedback_chatbot_box: gr.update(visible=False)}\n",
        "        def user(user_message, history):\n",
        "            is_sensitive, respond_message = guardian.filter(user_message)\n",
        "            if is_sensitive:\n",
        "                bot_message = respond_message\n",
        "            else:\n",
        "                bot_message = chatgpt_chain.predict(human_input=user_message)\n",
        "            history.append((user_message, bot_message))\n",
        "            return \"\", history,gr.update(visible=True)\n",
        "        def reset():\n",
        "          chatgpt_chain.memory.clear()\n",
        "          print(\"clear!\")\n",
        "        feedback_chatbot_submit.click(fn=save_up, inputs=[chatbot,chatbot_radio,feedback_chatbot], outputs=[feedback_chatbot_ok,feedback_chatbot_box,], queue=False)\n",
        "        clear.click(reset, None, chatbot, queue=False)\n",
        "        submit_event = msg.submit(fn=user, inputs=[msg, chatbot], outputs=[msg, chatbot,feedback_chatbot_box], queue=True)\n",
        "        submit_click_event = submit.click(fn=user, inputs=[msg, chatbot], outputs=[msg, chatbot,feedback_chatbot_box], queue=True)\n",
        "    with gr.Tab(\"ChatBot without LangChain\"):\n",
        "        chatbot2 = gr.Chatbot()\n",
        "        msg2 = gr.Textbox(label=\"Your sentence here... (press enter to submit)\")\n",
        "        with gr.Column():\n",
        "            with gr.Column(visible=False) as feedback_chatbot_box2:\n",
        "                chatbot_radio2 = gr.Radio(\n",
        "                    [\"Good\", \"Bad\", \"Report\"], label=\"Do you think about the chat?\"\n",
        "                )\n",
        "                feedback_chatbot2 = gr.Textbox(placeholder=\"Feedback chatbot\",show_label=False, lines=4)\n",
        "                feedback_chatbot_submit2 = gr.Button(\"Submit Feedback\")\n",
        "            with gr.Row(visible=False) as feedback_chatbot_ok2:\n",
        "                gr.Markdown(\"Thank you for feedback.\")\n",
        "        \n",
        "        def user2(user_message, history):\n",
        "            return \"\", history + [[user_message, None]]\n",
        "        def bot2(history):\n",
        "            _bot = list2prompt(history)\n",
        "            bot_message = gen_chatbot_old(_bot)\n",
        "            history[-1][1] = bot_message\n",
        "            return history,gr.update(visible=True)\n",
        "        def save_up2(history,choice,feedback):\n",
        "            _bot = list2prompt(history)\n",
        "            x=False\n",
        "            if choice==\"Good\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=1,feedback=feedback,name_model=name_model+\"-chat_old\")\n",
        "            elif choice==\"Bad\":\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=0,feedback=feedback,name_model=name_model+\"-chat_old\")\n",
        "            else:\n",
        "              x=sumbit_data(save=\"chat\",prompt=_bot,vote=3,feedback=feedback,name_model=name_model+\"-chat_old\")\n",
        "            return {feedback_chatbot_ok2: gr.update(visible=True),feedback_chatbot_box2: gr.update(visible=False)}\n",
        "        msg2.submit(user2, [msg2, chatbot2], [msg2, chatbot2], queue=True).then(bot2, chatbot2, [chatbot2,feedback_chatbot_box2])\n",
        "        feedback_chatbot_submit2.click(fn=save_up2, inputs=[chatbot2,chatbot_radio2,feedback_chatbot2], outputs=[feedback_chatbot_ok2,feedback_chatbot_box2], queue=False)\n",
        "        clear2 = gr.Button(\"Clear\")\n",
        "        clear2.click(lambda: None, None, chatbot2, queue=False)\n",
        "demo.queue()\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "7lHnjEOU-UNB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
        "\n",
        "- [ü¶ú‚õìÔ∏è LangChain + üêò WangchanGLM](https://colab.research.google.com/github/PyThaiNLP/WangChanGLM/blob/main/demo/LangChain_WangchanGLM.ipynb)\n",
        "\n",
        "‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏ó‡πà‡∏≤‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏™‡πà‡∏á feedback ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏∏‡πà‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ü‡∏£‡∏µ"
      ],
      "metadata": {
        "id": "PdZG5m3gXICK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö üêò WangChanGLM ‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏≤‡∏á Facebook: [AIResearch.in.th](https://www.facebook.com/AIResearch.in.th) ‡∏Å‡∏±‡∏ö [PyThaiNLP](https://www.facebook.com/pythainlp)"
      ],
      "metadata": {
        "id": "u4gkgn7p5XgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‡∏Ç‡πà‡∏≤‡∏ß**\n",
        "\n",
        "‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏°‡∏≤‡∏û‡∏ö‡∏Å‡∏±‡∏ô‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°‡∏ô‡∏µ‡πâ!\n",
        "\n",
        "VISTEC AI Day 2023\n",
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 22 ‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏° 2566 13:30-16:30 ‡∏ô.\n",
        "The Synergy Hall, ‡∏ä‡∏±‡πâ‡∏ô 6, Energy Complex, Building C\n",
        "‡∏û‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡∏ä‡∏±‡πâ‡∏ô‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å VISTEC ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏ç‡πà\n",
        "‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏™‡∏π‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à\n",
        "‡∏û‡∏ö‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡∏à‡∏±‡∏¢ ‡πÅ‡∏•‡∏∞‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡∏™‡∏∏‡∏î‡∏•‡πâ‡∏≥:\n",
        "- VISAI.ai‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ AI ‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏Ñ‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à\n",
        "Wangchan Advance Industrial Labs: ‡πÅ‡∏•‡πá‡∏ö‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå ‡πÅ‡∏•‡∏∞‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå\n",
        "- WangchanX ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏ß‡πà‡∏≤ 1.2 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß WangChanGLM ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á generative ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤\n",
        "- VISTEC-Siriraj Frontier Research Center ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏ä‡∏±‡πâ‡∏ô‡πÅ‡∏ô‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï \n",
        "- SensAI ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏•\n",
        "‡∏°‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏û‡∏ö‡∏õ‡∏∞‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç AI ‡πÅ‡∏•‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£‡∏™‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ä‡∏°‡∏ö‡∏π‡∏ò‡πÑ‡∏î‡πâ‡∏ü‡∏£‡∏µ\n",
        "‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà https://forms.office.com/r/y2nbppaG9g \n",
        "\n",
        "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà wachirawish_pro@vistec.ac.th\n"
      ],
      "metadata": {
        "id": "gPLfnj8l5967"
      }
    }
  ]
}
