{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07edeb1f",
   "metadata": {},
   "source": [
    "# Process Alpaca English and Translated Thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d4996d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 13:13:06.591524: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 13:13:07.663430: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2023-04-01 13:13:07.663550: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2023-04-01 13:13:07.663564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    AdamW,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from deepspeed.runtime.lr_schedules import WarmupDecayLR\n",
    "from typing import Optional, Union, List, Dict, Any\n",
    "import evaluate\n",
    "from dataclasses import dataclass, field\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "import multiprocessing\n",
    "import copy\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347ac1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #original\n",
    "# with open('../data_large/alpaca_data.json','r') as f:\n",
    "#     data = json.load(f)\n",
    "# dset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e653cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70686c7dc5f1400ea11bb129c3293db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration yahma--alpaca-cleaned-3d288cdc05baa98a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/yahma--alpaca-cleaned to /home/charipol/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-3d288cdc05baa98a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16df52207534516b7396bfe7e51ac87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9e2357faa642ccaa1c34a0b87ecc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/22.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afed117b0a774c04bc74695ce805f31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/charipol/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-3d288cdc05baa98a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d5a864dfbd46d1a74270e172d1876d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 51711\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('yahma/alpaca-cleaned')\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d61e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 30502], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/xglm-7.5B')\n",
    "tokenizer('hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0b2441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df86abdca4f349bd8d15eae70ffba2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51711 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_sft(example):\n",
    "    if example['input']!=\"\":\n",
    "        example['text'] = f\"<human>: {example['instruction']} <context>: {example['input']} <bot>: {example['output']}\"\n",
    "        example['has_context'] = 1\n",
    "    else:\n",
    "        example['text'] = f\"<human>: {example['instruction']} <bot>: {example['output']}\"\n",
    "        example['has_context'] = 0\n",
    "    example['nb_tokens'] = len(tokenizer(example['text'])['input_ids'])\n",
    "    return example\n",
    "\n",
    "dset_sft = dset.map(preprocess_sft,\n",
    "                remove_columns=['instruction','input','output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bece7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000     14.00\n",
       "0.001     17.00\n",
       "0.002     19.00\n",
       "0.003     19.00\n",
       "0.004     20.00\n",
       "          ...  \n",
       "0.995    357.45\n",
       "0.996    378.00\n",
       "0.997    405.87\n",
       "0.998    439.58\n",
       "0.999    516.16\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#99.9% are less than 512 tokens\n",
    "pd.Series(dset_sft['train']['nb_tokens']).quantile([i/1000 for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ac0e5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 37 examples are over 512; we decide to remove it first\n",
    "(pd.Series(dset_sft['train']['nb_tokens'])>512).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561aff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbbbe9bc73c44789d3a6540278bbb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'has_context'],\n",
       "        num_rows: 51656\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter down\n",
    "dset_sft['train'] = dset_sft['train'].filter(lambda example: example[\"nb_tokens\"]<=512)\\\n",
    "            .remove_columns(['nb_tokens'])\n",
    "\n",
    "dset_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6c8b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<human>: Generate a summary of the given article. <context>: Between 1989 and 2019, the size of the U.S. economy, measured in terms of the gross domestic product (GDP), more than doubled. The growth was faster than in the prior two decades, driven in part by the dramatic expansion of the information technology sector in the 1990s and early 2000s, gains in worker productivity and strong consumer demand, and government investments in research and development. <bot>: The U.S. economy more than doubled in size between 1989 and 2019, largely due to the rise of the information technology sector, improvements in worker productivity and consumer demand, and government investments. This growth was faster than in the prior two decades.',\n",
       " 'has_context': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_sft['train'][120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4a85453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset_sft = dset_sft['train'].train_test_split(test_size=1000, seed=125)\n",
    "# dset_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36df2305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a2749aff424edc86f0197ee4101481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset_sft.push_to_hub('pythainlp/alpaca_cleaned_en_sft', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00f3cd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0792890e149484cb66aa67e4e6ad8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/424 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration pythainlp--alpaca_cleaned_en_sft-3609a06dc02a1421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/charipol/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--alpaca_cleaned_en_sft-3609a06dc02a1421/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4be2233bd24713865fd97c0de003bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fce62e75ce942a0928eeb8f992abfe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299c27492a7947d7bf3f573ffdc68256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/charipol/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--alpaca_cleaned_en_sft-3609a06dc02a1421/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b5779041f749cf85ea71d60f8f20d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset_sft = load_dataset('pythainlp/alpaca_cleaned_en_sft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ec970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b374d96e6ecb4851baa98a799956c73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851fbd28aceb43db9179022037540852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the value-head model and tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/xglm-564M')\n",
    "model = AutoModelForCausalLM.from_pretrained('facebook/xglm-564M')\n",
    "\n",
    "# Preprocess the dataset.\n",
    "def mask_labels(l, context_cue, human_cue, bot_cue):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(l):\n",
    "        if (l[i:i+len(human_cue)] == human_cue)|((l[i:i+len(context_cue)] == context_cue)):\n",
    "            while l[i:i+len(bot_cue)] != bot_cue:\n",
    "                result.append(-100)\n",
    "                i += 1\n",
    "        else:\n",
    "            result.append(l[i])\n",
    "            i += 1\n",
    "    return result\n",
    "        \n",
    "def preprocess_function(example):\n",
    "    tokenized_qa = tokenizer(example['text']+tokenizer.eos_token, \n",
    "                            truncation=True, \n",
    "                            padding=\"max_length\",\n",
    "                            max_length=512,\n",
    "                            add_special_tokens=False\n",
    "                            )\n",
    "    labels = copy.deepcopy(tokenized_qa['input_ids'])\n",
    "    labels = mask_labels(labels, \n",
    "              tokenizer('<context>:', add_special_tokens=False)['input_ids'],\n",
    "              tokenizer('<human>:', add_special_tokens=False)['input_ids'],\n",
    "              tokenizer('<bot>:', add_special_tokens=False)['input_ids']\n",
    "             )\n",
    "    labels = [-100 if i==tokenizer.pad_token_id else i for i in labels]\n",
    "    return {\n",
    "        \"input_ids\": tokenized_qa[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_qa[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = dset_sft.map(preprocess_function, \n",
    "                      batched=False, \n",
    "                      num_proc=5, \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d054f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
