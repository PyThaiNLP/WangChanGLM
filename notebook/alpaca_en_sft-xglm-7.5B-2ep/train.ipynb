{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f57eee-df41-41fd-9f5c-6e2ba4009084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/mpi/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/lib:/usr/local/mpi/lib:/usr/local/mpi/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('America/Los_Angeles')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "#os.environ[\"HF_DATASETS_CACHE\"]=\"/workspace/1195/\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'facebook/xglm-7.5B', \n",
    "    load_in_8bit=False, \n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/xglm-7.5B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9157f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/xglm_ok\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a4a7b3-dd3d-4f73-b985-f7f22676f7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "#  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "#    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0003f8cf-1ace-4ee7-8ce6-bc571ba31d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67eb6790-9044-438a-b902-e2d3d879e295",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8388608 || all params: 7501160448 || trainable%: 0.1118308034890337\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293e8322-992f-422b-af61-d26cf90847eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--alpaca_en_sft-8c0a443e01591af7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a70d84be0c4eb08e7b06f4a9ba5e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"pythainlp/alpaca_en_sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f6cc0df-cc8d-4706-80f6-1ff57b002985",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--alpaca_en_sft-8c0a443e01591af7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cc06846f1fe22927.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--alpaca_en_sft-8c0a443e01591af7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56b8769e18803a2e.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5addf116-f559-481f-ae95-94bf3e2f5a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<human>: Evaluate the following statement \" The world is better now compared to 10 years ago.\" <bot>: This statement is subjective, and depends on a person\\'s own opinion and perspective. There are some aspects of the world that are better now than 10 years ago, such as the advancement of technology and the increasing awareness about global climate change and sustainability. However, there are also aspects which are worse, such as the widening economic inequality and increasing social unrest in many countries. It is impossible to definitively say that the world as a whole is better or worse than it was 10 years ago.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf42525a-bbdf-476b-9fcf-9c1cb06e96a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwannaphong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/xglm_ok/wandb/run-20230331_031810-uy87dezd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wannaphong/huggingface/runs/uy87dezd' target=\"_blank\">rich-microwave-140</a></strong> to <a href='https://wandb.ai/wannaphong/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wannaphong/huggingface' target=\"_blank\">https://wandb.ai/wannaphong/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wannaphong/huggingface/runs/uy87dezd' target=\"_blank\">https://wandb.ai/wannaphong/huggingface/runs/uy87dezd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XGLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 4:18:49, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>1.757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.586200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=770, training_loss=1.6717456371753248, metrics={'train_runtime': 15555.7184, 'train_samples_per_second': 6.347, 'train_steps_per_second': 0.049, 'total_flos': 5.60407984939008e+17, 'train_loss': 1.6717456371753248, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=32,\n",
    "        per_device_eval_batch_size=2,\n",
    "        warmup_steps=100, \n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_strategy=\"epoch\", \n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03203ab-15d8-4e76-8b60-a6d1811fcad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23f6479ca8d42eb92a0be42fed7422c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6eb22533e14d478a9ac07c786ceb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pythainlp/alpaca_en_sft-xglm-7.5B-2ep/commit/dfec0ef5ae7142e8cbf4f3c2523e40469fdbc021', commit_message='Upload model', commit_description='', oid='dfec0ef5ae7142e8cbf4f3c2523e40469fdbc021', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"pythainlp/alpaca_en_sft-xglm-7.5B-2ep\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2630707d-c4e5-4589-a8a9-dc133ea37ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d52a7ee4c4f45e5a302260ca1b3382c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8814af53eb48da890011a893a17c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fea97538594ccc85a157371ddf57d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pythainlp/alpaca_en_sft-xglm-7.5B-2ep/commit/5b45c8892e609adfb9fa8486105851bde6aff898', commit_message='Upload tokenizer', commit_description='', oid='5b45c8892e609adfb9fa8486105851bde6aff898', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"pythainlp/alpaca_en_sft-xglm-7.5B-2ep\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15a515b9-54c7-48e6-bb67-07f84d1140e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_ep2/tokenizer_config.json',\n",
       " 'test_ep2/special_tokens_map.json',\n",
       " 'test_ep2/sentencepiece.bpe.model',\n",
       " 'test_ep2/added_tokens.json',\n",
       " 'test_ep2/tokenizer.json')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"test_ep2\")\n",
    "tokenizer.save_pretrained(\"test_ep2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57865369-b974-4fc9-90ed-3115187a3bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7035a8-1ab5-4119-9d80-5d14a0bde932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
