{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c820a48",
   "metadata": {},
   "source": [
    "## Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d82aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "import jsonlines\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35daa5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = glob.glob('../data_large/pantip-large/*.jsonl')\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5560ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "content_dfs = []\n",
    "comment_dfs = []\n",
    "for fname in tqdm(fnames):\n",
    "    with jsonlines.open(fname) as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            if 'comment' in obj.keys():\n",
    "                d = {\n",
    "                    'datasource_url': obj['datasource_url'],\n",
    "                    'content_id': obj['content_id'],\n",
    "                    'comment_id': obj['comment_id'],\n",
    "                    'order_comment': obj['order_comment'],\n",
    "                    'type':obj['type'],\n",
    "                    'comment': obj['comment'],\n",
    "                    'like_score': obj['like_score'],\n",
    "                    'feel_heart': obj['feel_heart'],\n",
    "                    'feel_laugh': obj['feel_laugh'],\n",
    "                    'feel_love': obj['feel_love'],\n",
    "                    'feel_sad': obj['feel_sad'],\n",
    "                    'feel_horror': obj['feel_horror'],\n",
    "                    'feel_wow': obj['feel_wow'],\n",
    "                }\n",
    "                comment_dfs.append(d)\n",
    "            elif ('content' in obj.keys()) and (obj['total_field'] > 1):\n",
    "                d = {\n",
    "                    'datasource_url': obj['datasource_url'],\n",
    "                    'content_id': obj['content_id'],\n",
    "                    'type':obj['type'],\n",
    "                    'title': obj['title'],\n",
    "                    'body': obj['content'],\n",
    "                    'total_field': obj['total_field'],\n",
    "                    'tags': obj['tags'],\n",
    "                    'like_score': obj['like_score'],\n",
    "                    'feel_heart': obj['feel_heart'],\n",
    "                    'feel_laugh': obj['feel_laugh'],\n",
    "                    'feel_love': obj['feel_love'],\n",
    "                    'feel_sad': obj['feel_sad'],\n",
    "                    'feel_horror': obj['feel_horror'],\n",
    "                    'feel_wow': obj['feel_wow'],\n",
    "                }\n",
    "                content_dfs.append(d)    \n",
    "\n",
    "comment_df = pd.DataFrame(comment_dfs)\n",
    "content_df = pd.DataFrame(content_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb6d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = content_df.drop_duplicates()\n",
    "#remove threads with different snapshots\n",
    "content_df['rnk'] = content_df.sort_values(['total_field',],ascending=[False]) \\\n",
    "             .groupby(['content_id']) \\\n",
    "             .cumcount() + 1\n",
    "content_df = content_df[content_df.rnk==1]\n",
    "content_df.to_csv('../data_large/content_df.csv', index=None)\n",
    "\n",
    "\n",
    "comment_df = comment_df.drop_duplicates()\n",
    "comment_df['rnk'] = comment_df.groupby(['content_id','comment_id']).cumcount() + 1\n",
    "comment_df = comment_df[comment_df.rnk==1]\n",
    "comment_df.to_csv('../data_large/comment_df.csv', index=None)\n",
    "\n",
    "\n",
    "comment_df = pd.read_csv('../data_large/comment_df.csv')\n",
    "content_df = pd.read_csv('../data_large/content_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b592c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if content deduplicated successfully\n",
    "content_df.shape, content_df.content_id.nunique(), content_df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if comment deduplicated successfully\n",
    "comment_df.shape, comment_df.comment_id.nunique(), comment_df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97c00495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3510036, 34), 935910)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = content_df.merge(comment_df, on='content_id')\n",
    "all_df = all_df.drop(['rnk_x','rnk_y','datasource_url_y'],1)\n",
    "filter out later sub-comments\n",
    "all_df = all_df[all_df.order_comment.map(lambda x: '-' not in str(x))]\n",
    "create interact count\n",
    "all_df['nb_interact'] = all_df[['like_score_y',\n",
    "       'feel_heart_y', 'feel_laugh_y', 'feel_love_y', 'feel_sad_y',\n",
    "       'feel_horror_y', 'feel_wow_y']].sum(1)\n",
    "\n",
    "only keep top 5 comments with highest interacts\n",
    "all_df['rnk'] = all_df.sort_values(['nb_interact','like_score_y','feel_heart_y'], \\\n",
    "             ascending=False) \\\n",
    "             .groupby(['content_id']) \\\n",
    "             .cumcount() + 1\n",
    "all_df = all_df[all_df.rnk<=5].drop('rnk',1)\n",
    "\n",
    "#filter less than 2 comment\n",
    "all_df = all_df[all_df.total_field>1]\n",
    "\n",
    "#pick only question in title\n",
    "question_signals = ['ใคร',\n",
    "                    'ทำไม',\n",
    "                    'อะไร',\n",
    "                    'ไหน',\n",
    "                    'ไหม','มั้ย',\n",
    "                    'เท่าไร','ไหร่','กี่',\n",
    "                    'อย่างไร','ยังไง',\n",
    "                    'หรือ','หรอ','เรอะ']\n",
    "all_df = all_df[all_df.title.map(lambda x: sum(i in x for i in question_signals)>0)]\n",
    "\n",
    "#remove edit artifacts\n",
    "all_df['title'] = all_df.title.map(lambda x: x.split('แก้ไขข้อความเมื่อ')[0])\n",
    "all_df['body'] = all_df.body.map(lambda x: x.split('แก้ไขข้อความเมื่อ')[0])\n",
    "all_df['comment'] = all_df.comment.map(lambda x: x.split('แก้ไขข้อความเมื่อ')[0])\n",
    "\n",
    "#replace URL\n",
    "import re\n",
    "def replace_url(text): return re.sub(r'http\\S+|www\\S+', '[URL]', text)\n",
    "\n",
    "all_df['title'] = all_df.title.map(replace_url)\n",
    "all_df['body'] = all_df.body.map(replace_url)\n",
    "all_df['comment'] = all_df.comment.map(replace_url)\n",
    "\n",
    "#remove spoiler tags\n",
    "all_df['body'] = all_df.body.map(lambda x: str(x).replace('[Spoil] คลิกเพื่อดูข้อความที่ซ่อนไว้',''))\n",
    "all_df['comment'] = all_df.comment.map(lambda x: str(x).replace('[Spoil] คลิกเพื่อดูข้อความที่ซ่อนไว้',''))\n",
    "\n",
    "#drop na\n",
    "all_df = all_df.dropna()\n",
    "\n",
    "count tokens\n",
    "tokenizer_mgpt = AutoTokenizer.from_pretrained('sberbank-ai/mGPT')\n",
    "\n",
    "#title\n",
    "title_tokens = tokenizer_mgpt(all_df.title.tolist())\n",
    "all_df['title_tokens'] = [len(i) for i in title_tokens.input_ids]\n",
    "\n",
    "#body\n",
    "batch_size = 100_000\n",
    "toks = []\n",
    "for i in tqdm(range(0, len(all_df), batch_size)):\n",
    "    body_tokens = tokenizer_mgpt(all_df.body.tolist()[i:i+batch_size])\n",
    "    toks+= [len(i) for i in body_tokens.input_ids]\n",
    "all_df['body_tokens'] = toks\n",
    "\n",
    "#comment\n",
    "batch_size = 100_000\n",
    "toks = []\n",
    "for i in tqdm(range(0, len(all_df), batch_size)):\n",
    "    comment_tokens = tokenizer_mgpt(all_df.comment.tolist()[i:i+batch_size])\n",
    "    toks+= [len(i) for i in comment_tokens.input_ids]\n",
    "all_df['comment_tokens'] = toks\n",
    "\n",
    "#filter comments\n",
    "#min tok 12 (10th percentile) to filter out ขอบคุณ\n",
    "#max tok 512 (97th percentile) to filter out too detailed answers\n",
    "all_df = all_df[(all_df.comment_tokens>=12)&(all_df.comment_tokens<=512)]\n",
    "\n",
    "#filter title\n",
    "#min tok 16 (5th percentile) to filter out super short titles e.g. ทำไม?\n",
    "#max tok 75 (99th percentile) to filter out too long titles with too many special characters/emojis\n",
    "all_df = all_df[(all_df.title_tokens>=12)&(all_df.title_tokens<=75)]\n",
    "\n",
    "#filter prompt\n",
    "#min tok 48 (5th percentile) to filter out questions that are too random\n",
    "#max tok 512 (90th percentile) just to fit 1024 prompt+demo\n",
    "all_df = all_df[(all_df.prompt_tokens>=48)&(all_df.prompt_tokens<=512)]\n",
    "\n",
    "#filter out thread with less than 2 comments\n",
    "more_than_one = all_df.content_id.value_counts().reset_index()\n",
    "more_than_one = more_than_one[more_than_one.content_id>1]\n",
    "more_than_one.columns = ['content_id','nb_comments']\n",
    "all_df = all_df.merge(more_than_one,on='content_id',how='inner')\n",
    "\n",
    "#sort by nb_interact then comment_tokens; topmost has label 1, otherwise 0\n",
    "all_df['rnk'] = all_df.sort_values(['nb_interact','comment_tokens'], ascending=[False,False]) \\\n",
    "             .groupby(['content_id']) \\\n",
    "             .cumcount() + 1\n",
    "all_df['preference_label'] = all_df['rnk'].map(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "#add strict labels for only threads that top comments indeed have more interacts (not just longer, no ties)\n",
    "rank_1 = all_df[all_df.rnk==1][['content_id','title','body','comment','nb_interact']]\n",
    "rank_2 = all_df[all_df.rnk==2][['content_id','comment','nb_interact']]\n",
    "rank_combined = rank_1.merge(rank_2, on='content_id')\n",
    "rank_combined['strict'] = 1\n",
    "rank_combined = rank_combined[rank_combined.nb_interact_x>rank_combined.nb_interact_y]\n",
    "rank_combined = rank_combined[['content_id','strict']]\n",
    "all_df = all_df.merge(rank_combined, on='content_id',how='left')\n",
    "all_df['strict'] = all_df.strict.fillna(0)\n",
    "\n",
    "all_df.to_csv('../data_large/all_df.csv', index=None)\n",
    "all_df.shape, all_df.content_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "389b85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from save\n",
    "all_df = pd.read_csv('../data_large/all_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b10b6e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3510036, 34), 935910)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.shape, all_df.content_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fd545",
   "metadata": {},
   "source": [
    "## Package as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "bf17a115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c4a81f5313492b9c2546f531f909b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/935910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save to jsonl\n",
    "import json\n",
    "\n",
    "with open('../data_large/php_json/php_universe.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for content_id in tqdm(all_df.content_id.unique()):\n",
    "        d = all_df[all_df.content_id==content_id].sort_values('preference_label', ascending=False)\n",
    "        thread_d = d[[\n",
    "        'content_id','tags','nb_comments',\n",
    "        'title_tokens','body_tokens','title','body','strict'\n",
    "        ]].drop_duplicates().to_dict(orient='records')[0]\n",
    "        comments = []\n",
    "        for i,row in d.iterrows():\n",
    "            comment_d = row[[\n",
    "                'order_comment',\n",
    "                'comment_tokens',\n",
    "                'comment',\n",
    "                'preference_label',\n",
    "                'nb_interact']].to_dict()\n",
    "            comments.append(comment_d)\n",
    "        thread_d['comments'] = comments\n",
    "        \n",
    "        #write to jsonl\n",
    "        json.dump(thread_d, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b9fe740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed61438ac54243fe817695ba279dddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "935910"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load from jsonl\n",
    "php_list = []\n",
    "with open('../data_large/php_json/php_data.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in tqdm(file):\n",
    "        item = json.loads(line.strip())\n",
    "        item['body'] = '' if item['body']!=item['body'] else item['body']\n",
    "        php_list.append(item)\n",
    "len(php_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd09544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content_id', 'tags', 'nb_comments', 'title_tokens', 'body_tokens', 'title', 'body', 'strict', 'comments'],\n",
       "    num_rows: 935910\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataset\n",
    "ds = Dataset.from_list(php_list)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "09450260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#push to hub\n",
    "ds.push_to_hub(\"pythainlp/php\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f07998f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e9699e1d7f40ebaea1a090dfed795a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration pythainlp--php-62efb1c37afc4be5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/pythainlp--php to /home/charipol/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--php-62efb1c37afc4be5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d22fef45664fe4be43c6c86bcdc7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a0fb84c84a4dbcaca057a6b20e17de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/172M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40455b57385b47c39fa5dfa2b512ad90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/174M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9770b148f4a54b16a2057c2130839d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/171M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d99f6fc42148bcabdc3d514a2d4650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626356649436426da5715c50c057d260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cecb82c3e44e95ae1edd01d95bad50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b42ef895f4b4a8f909678be37ae26ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/charipol/.cache/huggingface/datasets/pythainlp___parquet/pythainlp--php-62efb1c37afc4be5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d877ed54b7ff4cb3b835c93b88cc1d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content_id', 'tags', 'nb_comments', 'title_tokens', 'body_tokens', 'title', 'body', 'strict', 'comments'],\n",
       "        num_rows: 935910\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load from hub\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"pythainlp/php\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e2e591be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_id': 36188003,\n",
       " 'tags': 'Social Network,Twitter',\n",
       " 'nb_comments': 4,\n",
       " 'title_tokens': 20,\n",
       " 'body_tokens': 66,\n",
       " 'title': 'ส่งDMในทวิตไม่ได้ ทำยังไงดี?',\n",
       " 'body': 'DMในทวิตไม่ได้ค่ะ พอส่งมันก็ขึ้นว่า การดำเนินการนี้ได้รับการรายงานว่าน่าสงสัย  ต้องทำยังไงคะช่วยบอกที???',\n",
       " 'strict': 0.0,\n",
       " 'comments': [{'comment': 'จขกท.แก้ได้ยังค่ะ เราก็เปน ฝากด้วยนะคะ(แต่เราส่งไม่ได้แค่แอคเดียว นอกนั้นปกติค่ะ)',\n",
       "   'comment_tokens': 55,\n",
       "   'nb_interact': 0,\n",
       "   'order_comment': 'ความคิดเห็นที่ 3',\n",
       "   'preference_label': 1},\n",
       "  {'comment': 'ส่งหาใครคะ ดีเอ็มต้องโฟโลทั้งสองฝ่ายปะอะ',\n",
       "   'comment_tokens': 28,\n",
       "   'nb_interact': 0,\n",
       "   'order_comment': 'ความคิดเห็นที่ 1',\n",
       "   'preference_label': 0},\n",
       "  {'comment': 'ไม่ต้อง follow กันและกัน ก็ส่งได้นะส่งมาแล้วแต่ถ้าอีกฝ่าย block คุณยังไงก็ส่งไม่ได้',\n",
       "   'comment_tokens': 52,\n",
       "   'nb_interact': 0,\n",
       "   'order_comment': 'ความคิดเห็นที่ 2',\n",
       "   'preference_label': 0},\n",
       "  {'comment': 'แก้ได้ยังคะ?เราก็เป็น',\n",
       "   'comment_tokens': 15,\n",
       "   'nb_interact': 0,\n",
       "   'order_comment': 'ความคิดเห็นที่ 4',\n",
       "   'preference_label': 0}]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4270da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
